# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.3.4
#   kernelspec:
#     display_name: deep_rl_for_portfolio_optimization
#     language: python
#     name: deep_rl_for_portfolio_optimization
# ---

# We present the main results we obtained with Deep Reinforcement Learning on the three
# tractable cost models considered in the paper. 
# Êàë‰ª¨Â±ïÁ§∫‰∫ÜÊàë‰ª¨Âú®ËÆ∫Êñá‰∏≠ËÄÉËôëÁöÑ‰∏â‰∏™ÂèØÂ§ÑÁêÜÊàêÊú¨Ê®°Âûã‰∏ä‰ΩøÁî®Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ëé∑ÂæóÁöÑ‰∏ªË¶ÅÁªìÊûú„ÄÇ

# %matplotlib inline 
#Âú®jupyter notebook‰∏≠‰ΩøÁî®matplotlibÁªòÂõæ

# +
import os
import seaborn as sns #Áî®‰∫éÁªòÂà∂ÁÉ≠ÂäõÂõæÁöÑÂ∑•ÂÖ∑ÂåÖ
import matplotlib.pyplot as plt #Áî®‰∫éÁªòÂà∂ÂõæÂΩ¢ÁöÑÂ∑•ÂÖ∑ÂåÖ
import numpy as np #Áî®‰∫éÊï∞ÂÄºËÆ°ÁÆóÁöÑÂ∑•ÂÖ∑ÂåÖ
import pandas as pd #Áî®‰∫éÊï∞ÊçÆÂ§ÑÁêÜÁöÑÂ∑•ÂÖ∑ÂåÖ
from tqdm import tqdm_notebook #Áî®‰∫éÊòæÁ§∫ËøõÂ∫¶Êù°ÁöÑÂ∑•ÂÖ∑ÂåÖ
import warnings #Áî®‰∫éÂøΩÁï•Ë≠¶ÂëäÁöÑÂ∑•ÂÖ∑ÂåÖ

# local imports
from agent import Agent #ÂØºÂÖ•AgentÁ±ª
from env import Environment #ÂØºÂÖ•EnvironmentÁ±ª
from evaluation import test_models, plot_bars, plot_function #ÂØºÂÖ•test_models, plot_bars, plot_functionÂáΩÊï∞

##### Êú¨Êñá‰ª∂ÁöÑ‰∏ªË¶Å‰ΩúÁî®Â∞±ÊòØÊºîÁ§∫‰∏âÁßç‰∫§ÊòìÊ®°Âûã
# -

warnings.filterwarnings("ignore") #ÂøΩÁï•Ë≠¶Âëä

# + [markdown] toc-hr-collapsed=false
# # Linear trading costs with risk penalty
# -

# $$
# c(\pi_t, p_t) = -\pi_t p_t + \lambda \pi_t^2 + \psi | \pi_t - \pi_{t-1}|;\hspace{2mm}
# \lambda, \psi > 0
# $$

# ***Approximate Optimal Solution***

# for simplicity we will consider an approximate optimal solution which shape is
# parameterized making this solution easily found by a simple girdSearch. #‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁΩëÊ†ºÊêúÁ¥¢ÔºåÊâæÂà∞ÊúÄ‰Ω≥ÂèÇÊï∞
#ÂáΩÊï∞ÁöÑ‰∏ªË¶ÅÁÆóÊ≥ï 
# $$ f\left(\pi_{t-1}, p_t \right) = \begin{cases} \frac{1}{2\widetilde{\lambda}}\left(p_t
# - \widetilde{\psi} \right) - \pi_{t-1}; \hspace{2mm} p_t \ge \widetilde{\psi} +
# 2\widetilde{\lambda} \pi_{t-1} \\ 0 \hspace{13mm}; \hspace{2mm} -\widetilde{\psi} +
# 2\widetilde{\lambda} \pi_{t-1} \le p_t \le \widetilde{\psi} + 2\widetilde{\lambda}
# \pi_{t-1} \\ \frac{1}{2\widetilde{\lambda}}\left( p_t + \widetilde{\psi} \right)-
# \pi_{t-1}; \hspace{2mm} p_t \le -\widetilde{\psi} + 2\widetilde{\lambda} \pi_{t-1}
# \end{cases} $$
#
# Parameters $\widetilde{\lambda}, \widetilde{\psi}$ can be found with a gridSearch.

# ***Environment***
#
# We set our environment with the follwing parameters:
# $$
# \begin{cases}
# \theta = 0.1 \\
# \sigma = 0.1 \\
# T = 5000 \\
# \lambda = 0.3 \\
# \psi = 4
# \end{cases}
# $$

# +
# Environment parameters

SIGMA = 0.1 #Ê≥¢Âä®Áéá
THETA = 0.1 #OUËøáÁ®ãÁöÑÂèÇÊï∞
T = 5000 #Êó∂Èó¥ÈïøÂ∫¶
LAMBD = 0.3 #lambdaÁ≥ªÊï∞
PSI = 4 #psi ‰∫§ÊòìÁöÑÊÉ©ÁΩö„ÄÅÊàñËÄÖÊî∂Áõä
# -
# ÂàùÂßãÂåñ‰∏Ä‰∏™Environment
env = Environment(
    sigma=SIGMA, theta=THETA, T=T, lambd=LAMBD, psi=PSI, cost="trade_l1", scale_reward=1
)
#----------------------------------------------------------------------------------start ------------------------
### ***Optimal Solution*** Ê®°Âºè‰∏Ä: Grid,ÁΩëÊ†ºÊ®°Âºè
# ***GridSearch*** 
# Grid,ÁΩëÊ†ºÊ®°Âºè
#
# We perform a GridSearch to look for the optimal parameters $\widetilde{\lambda},
# \widetilde{\psi}$ using the average cumulative reward over $10$ episodes.
# Êàë‰ª¨ÊâßË°å‰∏Ä‰∏™GridSearchÊù•ÂØªÊâæÊúÄ‰Ω≥ÂèÇÊï∞$\widetilde{\lambda}, \widetilde{\psi}$Ôºå‰ΩøÁî®10‰∏™episodeÁöÑÂπ≥ÂùáÁ¥ØÁßØÂ•ñÂä±„ÄÇ
# +
# %%time
random_state = 1024 #ÈöèÊú∫ÁßçÂ≠ê
n_episodes = 10 #episodeÁöÑÊï∞Èáè 
rng = np.random.RandomState(random_state) #ÈöèÊú∫ÁßçÂ≠êÁîüÊàêÂô®
random_states = rng.randint(0, int(1e6), size=n_episodes) #ÁîüÊàêÈöèÊú∫ÁßçÂ≠ê

lambds = np.linspace(0.2, 0.6, 10) #‰∏ÄÁªÑlambds„ÄÇÊåáÂÆöÁöÑÈó¥ÈöîÂÜÖËøîÂõûÂùáÂåÄÈó¥ÈöîÁöÑÊï∞Â≠ó
psis = np.linspace(0.8, 1.2, 10) #‰∏ÄÁªÑpsisÔºåÁî®‰∫éÈÄâ‰∏æÊúÄ‰Ω≥ÂèÇÊï∞

# 1st dim: lambdas; 2nd dim: psis; 3rd dim: episodes
#ÂàùÂßãÂåñ‰∏Ä‰∏™Êî∂ÁõäËÅöÂêà
scores_episodes = np.empty((len(lambds), len(psis), n_episodes))
scores = np.empty((len(lambds), len(psis)))  # 1st dim: lambdas; 2nd dim: psis

#ÈÅçÂéÜlambds
for i, lambd in tqdm_notebook(list(enumerate(lambds))):
    for j, psi in tqdm_notebook(list(enumerate(psis))):
        #Ê®°Êãü10‰∏™episodeÔºåtest_applyËøîÂõûÊî∂Áõä
        score, score_episode, _, _, _ = env.test_apply(
            total_episodes=n_episodes, random_states=random_states, lambd=lambd, psi=psi
        )
        #Â°´ÂÖÖÊî∂ÁõäÈõÜÂêà
        scores[i, j] = score#Êî∂Áõä
        scores_episodes[i, j, :] = list(score_episode.values())#Êî∂ÁõäÈõÜÂêà
        # print('lambd=%.1f , psi=%.1f -> score=%.3f \n' % (lambd, psi, score))
# -
#‰ΩúÂõæ
plt.figure(figsize=(8, 6))
sns.heatmap(#ÁªòÂà∂ÁÉ≠ÂäõÂõæ
    pd.DataFrame(
        scores,
        index=pd.Index(np.round(lambds, 2), name=r"$\widetilde{\lambda}$"),
        columns=pd.Index(np.round(psis, 2), name=r"$\widetilde{\psi}$"),
    )
)
plt.show()#ÊòæÁ§∫ÂõæÁâá

# +
i_max = np.argmax(scores) // scores.shape[0] #ËøîÂõûÊúÄÂ§ßÂÄºÁöÑÁ¥¢Âºï
j_max = np.argmax(scores[i_max, :]) #ËøîÂõûÊúÄÂ§ßÂÄºÁöÑÁ¥¢Âºï

lambd_max, psi_max = lambds[i_max], psis[j_max] #ÊúÄÂ§ßÂÄºÁöÑlambdÂíåpsi
print("lambd_max=%.2f , psi_max=%.2f" % (lambd_max, psi_max))

#ËøáÁ®ãÔºö
#È¶ñÂÖàÔºåÈÄöËøáÂàõÂª∫‰∏Ä‰∏™ Agent ÂÆû‰æãÊù•ÂàùÂßãÂåñÊô∫ËÉΩ‰ΩìÔºåËÆæÁΩÆ‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑ‰∏Ä‰∫õÂèÇÊï∞Ôºå
#Êé•‰∏ãÊù•ÔºåËÆæÁΩÆ‰∫ÜËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑ‰∏Ä‰∫õÂèÇÊï∞
#Ë∞ÉÁî® agent.train() ÊñπÊ≥ïÂºÄÂßãËÆ≠ÁªÉÊô∫ËÉΩ‰Ωì„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢ÉËøõË°å‰∫§‰∫íÔºåÂπ∂Ê†πÊçÆ‰∫§‰∫íÁªèÈ™åÊù•Êõ¥Êñ∞Ëá™Â∑±ÁöÑÂèÇÊï∞„ÄÇ
#ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰ºö‰øùÂ≠òÊô∫ËÉΩ‰ΩìÁöÑÊ®°ÂûãÂèÇÊï∞Âà∞ÊåáÂÆöÁöÑË∑ØÂæÑÔºå‰ª•‰æøÂêéÁª≠ÊµãËØïÂíåÂàÜÊûê„ÄÇ
#Ë∞ÉÁî® test_models() ÊñπÊ≥ïÂØπËÆ≠ÁªÉÂæóÂà∞ÁöÑÊô∫ËÉΩ‰ΩìÊ®°ÂûãËøõË°åÊµãËØï„ÄÇÊµãËØïËøáÁ®ã‰∏≠‰ºöÂä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊ®°ÂûãÂèÇÊï∞ÔºåÂπ∂Âú®ÁéØÂ¢É‰∏≠ËøêË°å‰∏ÄÂÆöÊï∞ÈáèÁöÑÊµãËØïÂë®ÊúüÔºåËØÑ‰º∞Êô∫ËÉΩ‰ΩìÁöÑÊÄßËÉΩ„ÄÇ

#ËøõË°å‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÊµãËØïÔºåÊØîÂ¶ÇÂú®ÈöèÊú∫ÁßçÂ≠êÂíå‰∏Ä‰∫õÁâπÂÆöÁöÑÊµãËØïÂë®Êúü‰∏ãËøêË°åÊ®°ÂûãÔºå‰ª•ËØÑ‰º∞ÂÖ∂ÊÄßËÉΩ„ÄÇ
#ÊúÄÂêéÔºåÈÄöËøáÁªòÂõæÂíåËæìÂá∫ÁªìÊûúÊù•Â±ïÁ§∫ËÆ≠ÁªÉÂíåÊµãËØïÁöÑÁªìÊûúÔºåÊØîÂ¶ÇÊô∫ËÉΩ‰ΩìÁöÑÊî∂ÁõäÊõ≤Á∫ø„ÄÅÊåÅ‰ªìÊÉÖÂÜµÁ≠â„ÄÇ

# We find that $\widetilde{\lambda}=0.47, \widetilde{\psi}=0.93$

# ## Training
#ÈááÁî®DDPGÁÆóÊ≥ïËøõË°åËÆ≠ÁªÉ
# The agent is an actor critic architecture, both actor and critic networks are composed #
# of:
# - an inital fully connected layer of size $16$ with a reLU activation,#‰∏Ä‰∏™ÂÖ®ËøûÊé•Â±Ç
# - a hidden fully connected layer of size $16$ with a reLU activation and #‰∏Ä‰∏™ÈöêËóèÂ±ÇÂíåÊøÄÊ¥ªÂáΩÊï∞
# - a linear output layer of size $1$. #‰∏Ä‰∏™Á∫øÊÄßËæìÂá∫Â±Ç
#
# agent‰ΩøÁî®ÂÖ∂ÂΩìÂâçÁ≠ñÁï•Êé¢Á¥¢ÁéØÂ¢ÉÔºåÊ∑ªÂä†ÁöÑOUËøáÁ®ã„ÄÇÂèÇÊï∞‰∏∫$\theta=1, \sigma=1$ÔºåÂ∞ÜÁªèÈ™åÊîæÂÖ•Â§ßÂ∞è‰∏∫$10^6$ÁöÑÂõûÊîæÁºìÂÜ≤Âå∫Ôºå
# Âπ∂‰∏î‰ΩøÁî®‰ºòÂÖàÁªèÈ™åÈáçÊîæÔºåÊØè$50$‰∏™Êó∂Èó¥Ê≠•ÈááÊ†∑Â§ßÂ∞è‰∏∫$512$ÁöÑÊâπÊ¨°„ÄÇ
# The agent explores the environment with its current policy, an additive OU noise of
# parameters $\theta=1, \sigma=1$, it puts the experiences in a replay buffer of size
# $10^6$ and, we use Prioritized Experience Replay to sample a batch of size $512$ each
# $50$ time steps when we run a learning iteration.
#
# #agentÊòØ‰∏Ä‰∏™actor - criticÁªìÊûÑÔºåÁî±actorÁΩëÁªúÂíåcriticÁΩëÁªúÁªÑÊàê
# Before the training process starts, we run a pretraining phase and fill the replay
# buffer with $1000$ experiences generated by exploring the environment with the initial
# Actor network plus the additional OU noise. 
# #Âú®ËÆ≠ÁªÉËøáÁ®ãÂºÄÂßã‰πãÂâçÔºåÊàë‰ª¨ËøêË°å‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºå
# Âπ∂‰ΩøÁî®ÂàùÂßãActorÁΩëÁªúÂä†‰∏äÈ¢ùÂ§ñÁöÑOUÂô™Â£∞Êé¢Á¥¢ÁéØÂ¢ÉÁîüÊàê1000‰∏™ÁªèÈ™åÂ°´ÂÖÖÂõûÊîæÁºìÂÜ≤Âå∫„ÄÇ

# +
# Agent parameters
MAX_STEPS = 5000 #ÊúÄÂ§ßÊ≠•Êï∞
MEMORY_TYPE = "prioritized" #Á±ªÂûã
SLIDING = "oldest" #ÊªëÂä®Á™óÂè£
BATCH_SIZE = 2 ** 9 #Ê≠•Èïø
MAX_SIZE = int(1e6) #ÊúÄÂ§ßÂ§ßÂ∞è

# Training parameters
TOTAL_EPISODES = 200  # ÊÄªÁöÑËÆ≠ÁªÉÂë®ÊúüÊï∞„ÄÇÊØè‰∏™Âë®ÊúüÂåÖÊã¨‰ªéÁéØÂ¢ÉÂºÄÂßãÂà∞ÁªìÊùüÁöÑ‰∏ÄÁ≥ªÂàó‰∫§‰∫íÊ≠•È™§„ÄÇ
TOTAL_STEPS = 1000 # ÊØè‰∏™ËÆ≠ÁªÉÂë®Êúü‰∏≠ÁöÑÊÄªÊ≠•Êï∞„ÄÇÊØè‰∏ÄÊ≠•ÊòØÊô∫ËÉΩ‰Ωì‰∏éÁéØÂ¢É‰∫§‰∫íÁöÑ‰∏ÄÊ¨°„ÄÇ
FREQ = 10 #È¢ëÁéá ÊéßÂà∂Â§ö‰πÖ‰øùÂ≠ò‰∏ÄÊ¨°Ê®°ÂûãÂèÇÊï∞ÊàñËæìÂá∫ËÆ≠ÁªÉ‰ø°ÊÅØÁöÑÈ¢ëÁéá„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúËÆæÁΩÆ‰∏∫10ÔºåÂàôÊØè10‰∏™Âë®Êúü‰øùÂ≠ò‰∏ÄÊ¨°Ê®°ÂûãÂèÇÊï∞ÊàñËæìÂá∫‰∏ÄÊ¨°ËÆ≠ÁªÉ‰ø°ÊÅØ„ÄÇ
LEARN_FREQ = 50 #ÊéßÂà∂Â§ö‰πÖÊõ¥Êñ∞‰∏ÄÊ¨°Êô∫ËÉΩ‰ΩìÁöÑÂèÇÊï∞„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúËÆæÁΩÆ‰∏∫50ÔºåÂàôÊØè50‰∏™Ê≠•È™§Êõ¥Êñ∞‰∏ÄÊ¨°Êô∫ËÉΩ‰ΩìÁöÑÂèÇÊï∞„ÄÇ
TAU_ACTOR = 0.3 #Áî®‰∫éËΩØÊõ¥Êñ∞Ôºàsoft updateÔºâÁöÑÊó∂Èó¥Â∏∏Êï∞ÔºåÁî®‰∫éÊõ¥Êñ∞ Actor ÁΩëÁªúÁöÑÂèÇÊï∞„ÄÇËΩØÊõ¥Êñ∞ÊòØ‰∏ÄÁßçÊ∏êËøõÂú∞Â∞ÜÁõÆÊ†áÁΩëÁªúÁöÑÂèÇÊï∞Êõ¥Êñ∞‰∏∫ÂΩìÂâçÁΩëÁªúÂèÇÊï∞ÁöÑÊñπÊ≥ï„ÄÇ
TAU_CRITIC = 0.1 #Áî®‰∫éËΩØÊõ¥Êñ∞ÁöÑÊó∂Èó¥Â∏∏Êï∞ÔºåÁî®‰∫éÊõ¥Êñ∞ Critic ÁΩëÁªúÁöÑÂèÇÊï∞„ÄÇ„ÄÇ
LR_ACTOR = 1e-3 #Actor ÁΩëÁªúÁöÑÂ≠¶‰π†Áéá„ÄÇÂ≠¶‰π†ÁéáÊéßÂà∂‰∫ÜÂèÇÊï∞Âú®ÊØèÊ¨°Êõ¥Êñ∞Êó∂ÁöÑË∞ÉÊï¥Â§ßÂ∞è„ÄÇ
LR_CRITIC = 1e-2 # Critic ÁΩëÁªúÁöÑÂ≠¶‰π†Áéá„ÄÇ
WEIGHTS_DECAY_ACTOR = 0 #Actor ÁΩëÁªúÁöÑÊùÉÈáçË°∞ÂáèÁ≥ªÊï∞„ÄÇÊùÉÈáçË°∞ÂáèÁî®‰∫éÊÉ©ÁΩöÊ®°ÂûãÂèÇÊï∞ÁöÑÂ§ßÂ∞èÔºåÊúâÂä©‰∫éÈò≤Ê≠¢ËøáÊãüÂêà„ÄÇ
WEIGHTS_DECAY_CRITIC = 0 #Critic ÁΩëÁªúÁöÑÊùÉÈáçË°∞ÂáèÁ≥ªÊï∞„ÄÇ
FC1_UNITS_ACTOR = 16 #Actor ÁΩëÁªú‰∏≠Á¨¨‰∏Ä‰∏™ÂÖ®ËøûÊé•Â±ÇÁöÑÁ•ûÁªèÂÖÉÊï∞Èáè„ÄÇ
FC2_UNITS_ACTOR = 16 #Actor ÁΩëÁªú‰∏≠Á¨¨‰∫å‰∏™ÂÖ®ËøûÊé•Â±ÇÁöÑÁ•ûÁªèÂÖÉÊï∞Èáè„ÄÇ
FC1_UNITS_CRITIC = 16 #Critic ÁΩëÁªú‰∏≠Á¨¨‰∏Ä‰∏™ÂÖ®ËøûÊé•Â±ÇÁöÑÁ•ûÁªèÂÖÉÊï∞Èáè„ÄÇ
FC2_UNITS_CRITIC = 16 #Critic ÁΩëÁªú‰∏≠Á¨¨‰∫å‰∏™ÂÖ®ËøûÊé•Â±ÇÁöÑÁ•ûÁªèÂÖÉÊï∞Èáè

############################
#Ëøô‰∫õÂèÇÊï∞Âú®ËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†Ê®°ÂûãÊó∂ÈùûÂ∏∏ÈáçË¶ÅÔºåÂÆÉ‰ª¨ÊéßÂà∂‰∫ÜËÆ≠ÁªÉÁöÑÈÄüÂ∫¶„ÄÅÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºå
# Ëøô‰∫õÂèÇÊï∞ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÂíåÊ®°ÂûãËøõË°åË∞ÉÊï¥Âíå‰ºòÂåñÔºå‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÁöÑËÆ≠ÁªÉÁªìÊûú„ÄÇ
##########################
# +
# %%time
#ÂÆö‰πâ‰∏Ä‰∏™Êô∫ËÉΩ‰ΩìÔºåÂπ∂‰∏îÂàùÂßãÂåñÂØπÂ∫îÁöÑÂèÇÊï∞
agent = Agent(
    max_size=MAX_SIZE,
    max_step=MAX_STEPS,
    memory_type=MEMORY_TYPE,
    sliding=SLIDING,
    batch_size=BATCH_SIZE,
)
# Êñá‰ª∂Ë∑ØÂæÑÔºåÁî®‰∫éÊµãËØïÊàêÊú¨ ÂÆûÈ™åÁ∫øÊÄß‰∫§ÊòìÊàêÊú¨
path = "Experiment_linear_trading_cost_true_per/"
if not os.path.exists(path + "weights/"):
    os.makedirs(path + "weights/")
#ÂèëËµ∑ËÆ≠ÁªÉ
agent.train(
    env=env,
    total_episodes=TOTAL_EPISODES,
    tau_actor=TAU_ACTOR,
    tau_critic=TAU_CRITIC,
    lr_actor=LR_ACTOR,
    lr_critic=LR_CRITIC,
    weight_decay_actor=WEIGHTS_DECAY_ACTOR,
    weight_decay_critic=WEIGHTS_DECAY_CRITIC,
    total_steps=TOTAL_STEPS,
    weights=path + "weights/",
    freq=FREQ,
    fc1_units_actor=FC1_UNITS_ACTOR,
    fc2_units_actor=FC2_UNITS_ACTOR,
    fc1_units_critic=FC1_UNITS_CRITIC,
    fc2_units_critic=FC2_UNITS_CRITIC,
    learn_freq=LEARN_FREQ,
    plots=True,
    lambd=lambd_max,
    psi=psi_max,
    tensordir=path + "runs/", #tensorboardÊñá‰ª∂Ë∑ØÂæÑ
    mile=100, #ÊØè100Ê≠•‰øùÂ≠ò‰∏ÄÊ¨°Ê®°Âûã
    decay_rate=1e-6, #Â≠¶‰π†ÁéáË°∞ÂáèÁéá
)
# -

# ***Evaluation***
#
#Âú®10‰∏™Êñ∞ÁöÑÊµãËØïepisode‰∏äËØÑ‰º∞Ê®°ÂûãÔºåÂπ∂Â∞ÜÂÖ∂‰∏éËøë‰ººÊúÄ‰ºòËß£ËøõË°åÊØîËæÉ„ÄÇ
# We evaluate our models and compare them with the approximate optimal solution on $10$
# new test episodes.

path_weights = path + "weights/"
#score_cumsum:Á¥ØÁßØÂàÜÊï∞
#pnls:Êî∂Áõä
#positions:ÊåÅ‰ªì
scores, scores_episodes, scores_cumsum, pnls, positions = test_models(#ÊµãËØïÊ®°Âûã 
    path_weights, env, n_episodes=10, fc1_units=16, fc2_units=16
) #ËØÑ‰º∞Ê®°Âûã 

random_state = 1024 #
n_episodes = 10 #
rng = np.random.RandomState(random_state)
#random_stats:ÈöèÊú∫ÁßçÂ≠ê
random_states = rng.randint(0, int(1e6), size=n_episodes)
score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(# ËøêË°åÊµãËØïÊ®°Âûã
    total_episodes=n_episodes, random_states=random_states, lambd=lambd_max, psi=psi_max
)
scores[-1] = score #ÊúÄÂêé‰∏Ä‰∏™Êî∂ÁõäÂàÜÊï∞
scores_episodes[-1] = score_episode #ÊúÄÂêé‰∏Ä‰∏™Êî∂Áõä


# Given a predictor signal, we visualize both the positions taken by our agent and those
# taken by the approximate optimal solution, we find that the positions evolve very
# similarly .
#ÁªôÂÆöÈ¢ÑÊµãÂô®‰ø°Âè∑ÔºåÂ∞ÜÊô∫ËÉΩ‰ΩìÊâÄÂç†ÊçÆÁöÑ‰ΩçÁΩÆÂíåÈÇ£‰∫õ‰ΩçÁΩÆÂèØËßÜÂåñ ÈááÁî®Ëøë‰ººÊúÄ‰ºòËß£ÔºåÂèëÁé∞‰ΩçÁΩÆÁöÑÊºîÂèòÈùûÂ∏∏Áõ∏‰ºº
#‰ª•‰∏ã‰ª£Á†ÅÈÉΩÊòØ‰ΩúÂõæÁî®ÔºåÊ≤°ÊúâÂï•Âè∑ÁúãÁöÑÔºå
# +
env.reset(random_state=730001) #ÈáçÁΩÆÁéØÂ¢É
plt.figure(figsize=(15, 6)) #ÁªòÂà∂ÂõæÂÉè

plt.subplot(1, 2, 1)
plt.plot(positions[110][730001], label="DDPG", color="g")
plt.plot(positions_opt[730001], label="OPT", color="r")
plt.plot(env.signal[1:], label="signal$", color="y")
plt.xlim(300, 600)
plt.xlabel(r"$t$", fontsize=15)
plt.ylabel(r"$p_t, \pi_t$", fontsize=15)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(positions[110][730001], label="DDPG", color="g")
plt.plot(positions_opt[730001], label="OPT", color="r")
plt.xlim(300, 600)
plt.xlabel(r"$t$", fontsize=15)
plt.ylabel(r"$\pi_t$", fontsize=15)
plt.legend()

plt.savefig("positions_penalty.png")
plt.show()
# -

plot_bars(scores) # plot_barsÂáΩÊï∞ÁöÑ‰ΩúÁî®ÔºöÊ®°ÂûãËØÑ‰º∞ÂæóÂàÜÁöÑÊù°ÂΩ¢Âõæ

sorted_scores = sorted(scores.values(), reverse=True)#ÊéíÂ∫è scores
print("Optimal agent score   : %.2f" % sorted_scores[0])
print("Best DDPG agent score : %.2f" % sorted_scores[1])

path_weights = path + "weights/"
models_keys = [0, 10, 50, 90, 110]
plot_function(
    path_weights,
    env,
    models_keys,
    low=-4,
    high=4,
    lambd=lambd_max,
    psi=psi_max,
    fc1_units=16,
    fc2_units=16,
)
#--------------------------------------------end-------------------------------------------------------
#--------------------------------------------start----------------------------------------------------
#Á∫øÊÄß‰∫§ÊòìÊàêÊú¨‰∏éÈ£éÈô©Á∫¶Êùü
# # Linear trading costs with risk constraint

# We can model the problem in two ways here: #Ôºõ‰∏§ÁßçÂª∫Ê®°ÂáΩÊï∞
#
# We keep the reward: $r(\pi_t, p_t) = \pi_t p_t - \psi | \pi_t - \pi_{t-1}|;\hspace{2mm}
# \lambda, \psi > 0$ and clip actions in the interval $\left[-M, M\right]$; this means
# that an agent does not get penalized for making action $a$ taking its position $\pi$
# beyond $M$, **i.e** $|\pi + a|>M$, since its next position $\pi'$ is s.t $|\pi'|=M$.
#
# The second way is harder, it does not involve clipping the positions but rather
# penalizes the agent for making actions taking its position outside the interval
# $\left[-M, M \right]$. We can do this by adding a smooth penalty to the reward, and we
# choose a $tanh$ barrier in the following way:
# $$ r(\pi_t, p_t) = \pi_t p_t - \psi | \pi_t - \pi_{t-1}| - \beta\left( tanh\left[
# \alpha\left( |\pi_t| - M - \gamma\right)\right] + 1\right);\hspace{2mm} \lambda, \psi,
# \alpha, \beta, \gamma > 0 $$
# This $tanh$ penalty is smooth and does not diverge for high $|\pi|$ values which makes
# it useful in training stabilization compared to a constant penalty or an exponential
# one.

# ***Optimal Solution***
#
# for simplicity we will consider an approximate optimal solution which shape is
# parameterized making this solution easily found by a simple grid-search.
#
# $$
# f\left(\pi_{t-1}, p_t \right) =
# \begin{cases}
# M - \pi_{t-1}; \hspace{2mm} p_t > \widetilde{q} \\
# 0 \hspace{13mm}; \hspace{2mm} |p_t| \le \widetilde{q} \\
# -M - \pi_{t-1}; \hspace{2mm} p_t < -\widetilde{q}
# \end{cases}
# $$
#
# Parameters $\widetilde{q}$ can be found with a gridSearch.

# ***Environment***
#
# First, we will work with the first setting and only use an experience generator to get
# some insight about how to properly explore and learn. Then we will work on both settings
# in the exploration-learning scheme training.
#
# We set our environment with the follwing parameters:
# $$
# \begin{cases}
# \theta = 0.1 \\
# \sigma = 0.1 \\
# T = 5000 \\
# M = 2 \\
# \psi = 4
# \end{cases}
# $$

# +
# Environment parameters


#Á¨¨‰∏ÄÁßçÂª∫Ê®°ÊñπÂºèÔºö
# Â•ñÂä±ÂáΩÊï∞ÂÆö‰πâ‰∏∫ $r(\pi_t, p_t) = \pi_t p_t - \psi | \pi_t - \pi_{t-1}|$„ÄÇ
# ËøôÊÑèÂë≥ÁùÄÊô∫ËÉΩ‰ΩìÂú®ÊØè‰∏ÄÊ≠•ÁöÑÂ•ñÂä±Áî±ÂΩìÂâçÊåÅ‰ªìÈáè $\pi_t$ ‰∏é‰ª∑Ê†º $p_t$ ÁöÑ‰πòÁßØ‰ª•ÂèäÊåÅ‰ªìÈáèÂèòÂåñÁöÑÁªùÂØπÂÄºÁöÑÊÉ©ÁΩöÁªÑÊàê„ÄÇ
# Ê≠§Â§ñÔºåÂä®‰ΩúÔºà‰∫§ÊòìÔºâË¢´ÈôêÂà∂Âú®Âå∫Èó¥ $[-M, M]$ ÂÜÖÔºåËøôÊÑèÂë≥ÁùÄÊô∫ËÉΩ‰Ωì‰∏ç‰ºöÂèóÂà∞Âú®ËØ•Âå∫Èó¥Â§ñËøõË°å‰∫§ÊòìÁöÑÊÉ©ÁΩö„ÄÇ
SIGMA = 0.1
THETA = 0.1
T = 5000
MAXPOS = 2
PSI = 4.0
# -

env = Environment(
    sigma=SIGMA,
    theta=THETA,
    T=T,
    psi=PSI,
    cost="trade_l1",
    squared_risk=False,
    max_pos=MAXPOS,
    clip=True,
    penalty="tanh",
)

alpha, beta, gamma = 10, 10, MAXPOS / 4

#tanh_penaltyÂáΩÊï∞ÁöÑ‰ΩúÁî®ÔºötanhÂπ≥ÊªëÊÉ©ÁΩöÈöúÁ¢ç
def tanh_penalty(pi): 
    # tanhÂπ≥ÊªëÊÉ©ÁΩöÈöúÁ¢ç
    return beta * (np.tanh(alpha * (abs(pi) - MAXPOS - gamma)) + 1)

plt.plot(np.linspace(0, 5), tanh_penalty(np.linspace(0, 5)), label="tanh_penalty")
plt.axvline(MAXPOS, color="r", label="MAXPOS")
plt.title("tanh smooth penalty barrier", fontsize=15)
plt.legend()
plt.savefig("tanh_barrier.png")
plt.show()

# +
# Agent parameters
MAX_STEPS = 5000
MEMORY_TYPE = "prioritized"
SLIDING = "oldest"
BATCH_SIZE = 2 ** 9 # batch size ÊâπÂ§ÑÁêÜÂ§ßÂ∞è
MAX_SIZE = int(1e6)

# Training parameters
TOTAL_EPISODES = 200  # set it to 501 for better convergence
TOTAL_STEPS = 10000
FREQ = 10
LEARN_FREQ = 50
TAU_ACTOR = 0.3
TAU_CRITIC = 0.1
LR_ACTOR = 1e-3
LR_CRITIC = 1e-2
WEIGHTS_DECAY_ACTOR = 0
WEIGHTS_DECAY_CRITIC = 0
FC1_UNITS_ACTOR = 16
FC2_UNITS_ACTOR = 16
FC1_UNITS_CRITIC = 16
FC2_UNITS_CRITIC = 16

# +
# %%time
agent = Agent(
    max_size=MAX_SIZE,
    max_step=MAX_STEPS,
    memory_type=MEMORY_TYPE,
    sliding=SLIDING,
    batch_size=BATCH_SIZE,
    alpha=0.6,
    theta=0.1,
)

path = "Experiment_maxpos/"
if not os.path.exists(path + "weights/"):
    os.makedirs(path + "weights/")

agent.train(
    env=env,
    total_episodes=TOTAL_EPISODES,
    tau_actor=TAU_ACTOR,
    tau_critic=TAU_CRITIC,
    lr_actor=LR_ACTOR,
    lr_critic=LR_CRITIC,
    weight_decay_actor=WEIGHTS_DECAY_ACTOR,
    weight_decay_critic=WEIGHTS_DECAY_CRITIC,
    total_steps=TOTAL_STEPS,
    weights=path + "weights/",
    freq=FREQ,
    fc1_units_actor=FC1_UNITS_ACTOR,
    fc2_units_actor=FC2_UNITS_ACTOR,
    fc1_units_critic=FC1_UNITS_CRITIC,
    fc2_units_critic=FC2_UNITS_CRITIC,
    learn_freq=LEARN_FREQ,
    plots=True,
    thresh=0.95,
    tensordir=path + "runs/",
    mile=100,
    decay_rate=1e-5,
)

# -

path_weights = path + "weights/"
scores, scores_episodes, scores_cumsum, pnls, positions = test_models(#ÂÆö‰πâÊµãËØïÊ®°Âûã
    path_weights, env, n_episodes=10, fc1_units=16, fc2_units=16
)

random_state = 1024
n_episodes = 10
rng = np.random.RandomState(random_state)
random_states = rng.randint(0, int(1e6), size=n_episodes)
score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply( #ÊµãËØï‰∫§Êòì
    total_episodes=n_episodes, random_states=random_states, thresh=0.95
)
scores[-1] = score
scores_episodes[-1] = score_episode

plot_bars(scores) #Êî∂ÁõäÊõ≤Á∫øÂõæ
#‰ΩúÂõæ
models_keys = [0, 20, 80, 120, 140]
plot_function( #ÁªòÂà∂ÂáΩÊï∞ path_weights
    path_weights,
    env,
    models_keys,
    low=-4,
    high=4,
    lambd=0.3,
    fc1_units=16,
    fc2_units=16,
    thresh=0.95,
    clip=True,
)

# +
env.reset(random_state=989115)
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
plt.plot(positions[140][989115], label="DDPG", color="g")
plt.plot(positions_opt[989115], label="OPT", color="r")
plt.plot(env.signal[1:], label="signal$", color="y")
plt.xlim(0, 300)
plt.xlabel(r"$t$", fontsize=15)
plt.ylabel(r"$p_t, \pi_t$", fontsize=15)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(positions[140][989115], label="DDPG", color="g")
plt.plot(positions_opt[989115], label="OPT", color="r")
plt.xlim(0, 300)
plt.ylim(-4, 4)
plt.xlabel(r"$t$", fontsize=15)
plt.ylabel(r"$\pi_t$", fontsize=15)
plt.legend()

plt.savefig("positions_constraint_per.png")
plt.show()
#------------------------------------------------Ê®°Âûã‰∏âÔºöÂΩ±ÂìçÊàêÊú¨Ôºàimpact costÔºâÁöÑÊñπÂΩ¢ÊàêÊú¨Ê®°Âûã ----------------------------------
# -
#ÂΩ±ÂìçÊàêÊú¨
#Âú®ËøôÁßçÊàêÊú¨Ê®°Âûã‰∏≠ÔºåÈô§‰∫Ü‰πãÂâçÊèêÂà∞ÁöÑÂ•ñÂä±ÂáΩÊï∞Â§ñÔºåËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂΩ±ÂìçÊàêÊú¨Ôºàimpact costÔºâÁöÑÊñπÂΩ¢ÊàêÊú¨Ê®°Âûã„ÄÇËÆ©ÊàëÊù•Ëß£Èáä‰∏Ä‰∏ãÔºö
#ÂΩ±ÂìçÊàêÊú¨ÊòØÊåáÂú®‰∫§ÊòìÂ∏ÇÂú∫‰∏≠‰π∞ÂçñËµÑ‰∫ßÊâÄÂ∏¶Êù•ÁöÑ‰ª∑Ê†ºÊ≥¢Âä®„ÄÇÂÆÉÊòØÁî±‰∫é‰∫§ÊòìÈáèÂ§ß„ÄÅÂ∏ÇÂú∫ÊµÅÂä®ÊÄß‰ΩéÁ≠âÂõ†Á¥†ÂØºËá¥ÁöÑ„ÄÇÂΩ±ÂìçÊàêÊú¨ÁöÑÂ≠òÂú®‰ΩøÂæó‰π∞Âçñ‰∫§Êòì‰∏çÂÜçÊòØÈõ∂ÂíåÊ∏∏ÊàèÔºåÂõ†‰∏∫‰∫§Êòì‰∏ÄÊñπÁöÑÊìç‰ΩúÂèØËÉΩ‰ºöÂΩ±ÂìçÂ∏ÇÂú∫‰ª∑Ê†ºÔºå‰ªéËÄåÂΩ±ÂìçÂè¶‰∏ÄÊñπÁöÑÊî∂Áõä„ÄÇ
#ËøôÁßçÊàêÊú¨Ê®°Âûã‰∏≠ÔºåËÄÉËôë‰∫Ü‰∏Ä‰∏™ÊñπÂΩ¢ÁöÑÂΩ±ÂìçÊàêÊú¨Ê®°Âûã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàêÊú¨Ê®°ÂûãÂÆö‰πâ‰∏∫Ôºö
#ùê∂(ùúãùë°,ùëéùë°)=ùëê‚ãÖùëéùë°2C(œÄt ,at )=c‚ãÖat2
#ÂÖ∂‰∏≠Ôºö$C(\pi_t, a_t)$ ÊòØÂú®ÊåÅ‰ªìÈáè‰∏∫ $\pi_t$ Êó∂ÊâßË°åÂä®‰Ωú $a_t$ ÊâÄ‰∫ßÁîüÁöÑÊàêÊú¨„ÄÇ$c$ ÊòØÂΩ±ÂìçÊàêÊú¨ÁöÑÁ≥ªÊï∞ÔºåË°®Á§∫ÊØèÂçï‰Ωç‰∫§ÊòìÈáèÁöÑÊàêÊú¨„ÄÇ
#ËøôÁßçÊàêÊú¨Ê®°ÂûãË°®ÊòéÔºåÂΩìÊô∫ËÉΩ‰ΩìÊâßË°å‰∏Ä‰∏™Âä®‰ΩúÊó∂ÔºåÊâÄ‰∫ßÁîüÁöÑÊàêÊú¨‰∏éÂä®‰ΩúÁöÑÂπ≥ÊñπÊàêÊ≠£ÊØî„ÄÇËøôÊÑèÂë≥ÁùÄÔºåÂä®‰ΩúÁöÑÂ§ßÂ∞èÂØπÊàêÊú¨ÁöÑÂΩ±ÂìçÊòØÈùûÁ∫øÊÄßÁöÑÔºåËÄå‰∏îÈöèÁùÄÂä®‰ΩúÁöÑÂ¢ûÂä†ËÄåÂ¢ûÂä†„ÄÇ
#ÂºïÂÖ•ÂΩ±ÂìçÊàêÊú¨Ê®°ÂûãÂèØ‰ª•Êõ¥ÁúüÂÆûÂú∞Ê®°ÊãüÈáëËûçÂ∏ÇÂú∫‰∏≠ÁöÑ‰∫§ÊòìÊàêÊú¨ÔºåÂπ∂‰∏îÊúâÂä©‰∫éÊô∫ËÉΩ‰ΩìÂ≠¶‰π†Êõ¥‰ºòÂåñÁöÑ‰∫§ÊòìÁ≠ñÁï•ÔºåËÄÉËôëÂà∞ÊàêÊú¨ÁöÑÂΩ±Âìç„ÄÇ
# # Squared impact model

# We consider the following cost model incorporating a squared impact cost:
#
# $$
# c(\pi_t, p_t) = -\pi_t p_t + \lambda \pi_t^2 + \phi \left( \pi_t -
# \pi_{t-1}\right)^2;\hspace{2mm} \lambda, \phi > 0 #ÂΩ±ÂìçÊàêÊú¨Ê®°ÂûãÁöÑÂÆö‰πâ
# $$
#
# Given the predictor signal $\left( p_t\right)_t$ the optimal position $\pi_t$ at time
# $t$ has the following form
# $$
# \pi_t = b\times EMA_a\left( p, t\right)
# $$
# Where $EMA_a\left( p, t\right)$ denotes the exponential moving average of preditor
# signal $p$ at time $t$ with a decay of $0 < a \le 1$ and $b > 0$
#
# We can now perform a grid-search over parameters $a$ and $b$

# ***Environment***
#
# We set our environment with the follwing parameters: #ËÆæÁΩÆÁéØÂ¢ÉÂèÇÊï∞
# $$
# \begin{cases}
# \theta = 0.1 \\
# \sigma = 0.1 \\
# T = 5000 \\
# \lambda = 0.3 \\
# \phi = 1
# \end{cases}
# $$

# +
# Environment parameters

SIGMA = 0.1
THETA = 0.1
T = 5000
LAMBD = 0.3
PSI = 1
# -
#ÂàùÂßãÂåñ‰∏Ä‰∏™Environment
env = Environment(
    sigma=SIGMA,
    theta=THETA,
    T=T,
    lambd=LAMBD,
    psi=PSI,
    cost="trade_l2",
    scale_reward=10,
)

# +
# Agent parameters
MAX_STEPS = 5000
MEMORY_TYPE = "prioritized"
SLIDING = "oldest"
BATCH_SIZE = 2 ** 9
MAX_SIZE = int(1e6)

# Training parameters
TOTAL_EPISODES = 200  # set it to 501 for better convergence
TOTAL_STEPS = 10000
FREQ = 10
LEARN_FREQ = 50
TAU_ACTOR = 0.3
TAU_CRITIC = 0.1
LR_ACTOR = 1e-3
LR_CRITIC = 1e-2
WEIGHTS_DECAY_ACTOR = 0
WEIGHTS_DECAY_CRITIC = 0
FC1_UNITS_ACTOR = 16
FC2_UNITS_ACTOR = 16
FC1_UNITS_CRITIC = 16
FC2_UNITS_CRITIC = 16

# +
# %%time
#ËÆæÁΩÆsquared_cost Ë∑ØÂæÑ
path = "Experiment_squared_cost/"
if not os.path.exists(path + "weights/"):
    os.makedirs(path + "weights/")

agent = Agent(
    max_size=MAX_SIZE,
    max_step=MAX_STEPS,
    memory_type=MEMORY_TYPE,
    sliding=SLIDING,
    batch_size=BATCH_SIZE,
    alpha=0.6,
    theta=0.1,
)
# agent = Agent(max_size = MAX_SIZE, max_step=MAX_STEPS, memory_type=MEMORY_TYPE,
# sliding=SLIDING, batch_size=BATCH_SIZE, alpha=0.6, point_max=2, n_points=5)
agent.train(
    env=env,
    total_episodes=TOTAL_EPISODES,
    tau_actor=TAU_ACTOR,
    tau_critic=TAU_CRITIC,
    lr_actor=LR_ACTOR,
    lr_critic=LR_CRITIC,
    weight_decay_actor=WEIGHTS_DECAY_ACTOR,
    weight_decay_critic=WEIGHTS_DECAY_CRITIC,
    total_steps=TOTAL_STEPS,
    weights=path + "weights/",
    freq=FREQ,
    fc1_units_actor=FC1_UNITS_ACTOR,
    fc2_units_actor=FC2_UNITS_ACTOR,
    fc1_units_critic=FC1_UNITS_CRITIC,
    fc2_units_critic=FC2_UNITS_CRITIC,
    learn_freq=LEARN_FREQ,
    plots=True,
    lambd=0.33,
    psi=0.47,
    tensordir=path + "runs/",
    mile=100,
    decay_rate=1e-5,
)
# -

random_state = 1024
n_episodes = 10
rng = np.random.RandomState(random_state)
random_states = rng.randint(0, int(1e6), size=n_episodes)
score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(
    total_episodes=n_episodes, random_states=random_states, lambd=0.33, psi=0.47
)

path_weights = path + "weights/"
scores, scores_episodes, scores_cumsum, pnls, positions = test_models(
    path_weights, env, n_episodes=10, fc1_units=16, fc2_units=16
)

random_state = 1024
n_episodes = 10
rng = np.random.RandomState(random_state)
random_states = rng.randint(0, int(1e6), size=n_episodes)
score, score_episode, scores_cumsum_opt, pnls_opt, positions_opt = env.test_apply(
    total_episodes=n_episodes, random_states=random_states, thresh=0.95
)
scores[-1] = score
scores_episodes[-1] = score_episode

plot_bars(scores)

models_keys = [0, 40, 60, 100, 120]
plot_function(
    path_weights,
    env,
    models_keys,
    low=-4,
    high=4,
    lambd=0.3,
    fc1_units=16,
    fc2_units=16,
    thresh=0.95,
    clip=True,
)
